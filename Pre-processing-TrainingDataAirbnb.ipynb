{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4299a26b-a495-4ef9-b5f2-24e0fdbd4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /opt/miniconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (2.3.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (2.3.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (1.16.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/miniconda3/lib/python3.13/site-packages (from category_encoders) (0.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.13/site-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/miniconda3/lib/python3.13/site-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419c6a21-2e9d-427f-9d67-486f026877e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Airbnb processing and training Data\n",
    "\n",
    "%reset_selective -f regex\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c640e18-6642-4001-86b0-3f2adf6b9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/juanreyes/Downloads/cleaned_airbnb_cdmx_2024_prediction_data.csv\"\n",
    "mexico_airbnb_df = pd.read_csv(\"/Users/juanreyes/Downloads/cleaned_Airbnb_prediction_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f75b1a4-7e56-46d1-9119-5a3b76ed7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                                 Non-Null Count  Dtype\n",
      "---  ------                                 --------------  -----\n",
      " 0   disponibility_cat                      1000 non-null   int64\n",
      " 1   min_nights_cat                         1000 non-null   int64\n",
      " 2   average_measure                        1000 non-null   int64\n",
      " 3   supermarket                            1000 non-null   int64\n",
      " 4   name                                   1000 non-null   int64\n",
      " 5   host_name                              1000 non-null   int64\n",
      " 6   neighbourhood                          1000 non-null   int64\n",
      " 7   room_type                              1000 non-null   int64\n",
      " 8   price_category                         1000 non-null   int64\n",
      " 9   calculated_host_listings_count         1000 non-null   int64\n",
      " 10  hospital                               1000 non-null   int64\n",
      " 11  university                             1000 non-null   int64\n",
      " 12  subway                                 1000 non-null   int64\n",
      " 13  restaurant                             1000 non-null   int64\n",
      " 14  park                                   1000 non-null   int64\n",
      " 15  security_index                         1000 non-null   int64\n",
      " 16  demand_index                           1000 non-null   int64\n",
      " 17  neighbourhood_area                     1000 non-null   int64\n",
      " 18  density_hospital                       1000 non-null   int64\n",
      " 19  density_supermarket                    1000 non-null   int64\n",
      " 20  density_park                           1000 non-null   int64\n",
      " 21  density_university                     1000 non-null   int64\n",
      " 22  density_subway                         1000 non-null   int64\n",
      " 23  density_restaurant                     1000 non-null   int64\n",
      " 24  dis_to_tourist_point_per_neighborhood  1000 non-null   int64\n",
      " 25  income_per_month                       1000 non-null   int64\n",
      "dtypes: int64(26)\n",
      "memory usage: 203.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numeric features\n",
    "categorical_low_card = ['disponibility_cat', 'min_nights_cat', 'average_measure', 'supermarket']\n",
    "categorical_high_card = ['name', 'host_name', 'neighbourhood', 'room_type', 'price_category']\n",
    "numeric_features = ['calculated_host_listings_count', 'hospital', 'university', 'subway', 'restaurant', 'park', 'security_index', 'demand_index', 'neighbourhood_area', 'density_hospital', 'density_supermarket', 'density_park', 'density_university', 'density_subway', 'density_restaurant', 'dis_to_tourist_point_per_neighborhood'\n",
    "]\n",
    "target_variable = 'income_per_month'\n",
    "\n",
    "# Preview data to confirm structure\n",
    "mexico_airbnb_df[categorical_low_card + categorical_high_card + numeric_features + [target_variable]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07865383-2238-4248-a191-82c9d6b376d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create dummy variables for low-cardinality categorical features\n",
    "categorical_low_card = ['disponibility_cat', 'min_nights_cat', 'average_measure', 'supermarket']\n",
    "mexico_airbnb_df_dummies = pd.get_dummies(mexico_airbnb_df, columns=categorical_low_card, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30a4da-ca97-4283-a0ac-86b710129aca",
   "metadata": {},
   "source": [
    "In the code above we created a Dummy Variables for Low-Cardinality Categorical Features and Low-cardinality categorical variables (e.g. disponibility_cat, min_nights_cat) these were also one-hot encoded using the pd.get_dummies() method in order to convert them into binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09963392-1807-4c18-80b0-0b41ba740e30",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Step 2: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "mexico_airbnb_df_dummies[numeric_features] = scaler.fit_transform(mexico_airbnb_df_dummies[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834c67f-cedc-4d81-81e4-3974b8e227e4",
   "metadata": {},
   "source": [
    "In step 2, we targeted encoded High-Cardinality Categorical Features such as: name, host_name, neighbourhood, room_type and price_category, which can introduce sparsity and noise when one-hot encoded.\n",
    "Due to this we applied Target Encoding, which replaces each category with the mean of the target variable (monthly_income) within that category. The goal of this is to reduce dimensionality and mandating predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb3f61e5-8f84-4469-9ade-9e681c473d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Target Encode High-Cardinality Features\n",
    "# Target encoding helps with features like site_id, app_id, device_model.\n",
    "#Using category_encoders.TargetEncoder, we replace each category with the mean of the target (click) for that category.\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Instantiate target encoder\n",
    "target_encoder = ce.TargetEncoder(cols=['name', 'host_name', 'neighbourhood', 'room_type', 'price_category'])\n",
    "\n",
    "# Fit and transform\n",
    "Airbnb_df_encoded = target_encoder.fit_transform(mexico_airbnb_df_dummies, mexico_airbnb_df['income_per_month'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd91787-a427-4ced-8fee-ddf047c3c5ac",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "In the above step: We deployed Target Encoding for High-Cardinality Features\n",
    "Some categorical variables in this dataset, such as name, host_name, 'neighbourhood, room_type, price_category, have a large number of unique categories (high cardinality). If we use one-hot encoding on these it will create a sparse dataset and might introduce noise or overfitting in out training models.\n",
    "To address this we used Target Encoding via category_encoders.TargetEncoder, which replaces each category with the mean of the target variable (income_per_month) for that category. This approach reduces dimensionality and preserves predictive information.\n",
    "This step is crucial to hep improve model performance when dealing with high-cardinality categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b483775-400c-4ad8-b7d0-2da0433aea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:  Standard Scale the Numeric Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_features = ['calculated_host_listings_count', 'hospital', 'university', 'subway', 'restaurant', 'park', 'security_index', 'demand_index', 'neighbourhood_area', 'density_hospital', 'density_supermarket', 'density_park', 'density_university', 'density_subway', 'density_restaurant', 'dis_to_tourist_point_per_neighborhood'\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "Airbnb_df_encoded[numeric_features] = scaler.fit_transform(Airbnb_df_encoded[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe4142-eef0-46a4-bba6-6bb516b63bb2",
   "metadata": {},
   "source": [
    "In step 4 we used Standard Scaling to optimize convergence of Logistic Regression or Gradient Boosting. This was done to make sure all numeric features contribute equally to the model. \n",
    "We used StandardScaler from sklearn.preprocessing to transform each numeric feature (calculated_host_listings_count', 'hospital', 'university', 'subway', 'restaurant', 'park', 'security_index', 'demand_index', 'neighbourhood_area', 'density_hospital', 'density_supermarket', 'density_park', 'density_university', 'density_subway', 'density_restaurant', 'dis_to_tourist_point_per_neighborhood') so that they have a mean of 0 and standard deviation of 1. This will prevent features with larger magnitudes from dominating the model training process.\n",
    "Note that only numeric features are standardized—categorical features (including dummy or encoded ones) are not scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79cd58f3-41f1-452a-8600-c38a072db40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (800, 66)\n",
      "Test set shape: (200, 66)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X and y\n",
    "X = Airbnb_df_encoded.drop(columns=['income_per_month'])\n",
    "y = Airbnb_df_encoded['income_per_month']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Optional: Check shape\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dadd8c-9d02-4226-b091-852675aafe4d",
   "metadata": {},
   "source": [
    "In step 5 we split our data into training and testing sets to evaluate our model performance fairly and avoid data leakage. We split our datasets into the following subsets.\n",
    "Training Set: Used to train the machine learning model.\n",
    "Testing Set: Used to evaluate the model’s performance on unseen data.\n",
    "We used the train_test_split() function from sklearn.model_selection with the following configurations:\n",
    "test_size=0.2: meaning 20% of the data is reserved for testing.\n",
    "random_state=42: which, ensures reproducibility of the split.\n",
    "stratify=y: Maintains the original class distribution\n",
    "These ensure the model is trained on a representative subset of the data and evaluated on an equally balanced holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6aefe0ba-ec58-4f09-8fc9-5aee510e2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw string (prefix with 'r') to avoid issues with backslashes\n",
    "## Save datasets in the processed data folder\n",
    "X_train.to_csv(\"/Users/juanreyes/Desktop/DataScienceGuidedCapstone/Springboard/CapstoneTwo_AirbnbPrediction/data/Processed/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/juanreyes/Desktop/DataScienceGuidedCapstone/Springboard/CapstoneTwo_AirbnbPrediction/data/Processed/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/juanreyes/Desktop/DataScienceGuidedCapstone/Springboard/CapstoneTwo_AirbnbPrediction/data/Processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/juanreyes/Desktop/DataScienceGuidedCapstone/Springboard/CapstoneTwo_AirbnbPrediction/data/Processed/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497827d-30ee-41ac-92f6-277c7b4d7a5f",
   "metadata": {},
   "source": [
    "In this last step we save our processed datasets as csv files to reuse in our modeling phase. The data is saved as follows: X_train, X_test, y_train and y_test.\n",
    "\n",
    "In the following step we will use our preprocessed datasets to train and evaluate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48259dd-373d-4504-b43e-3215e208d499",
   "metadata": {},
   "source": [
    "Feature Type Justification\n",
    "\n",
    "In this notebook we identified categorical and continuous features through data types and domain understanding.\n",
    "Categorical Features: 'disponibility_cat', 'min_nights_cat', 'average_measure', 'supermarket' have a small number of unique values and are encoded using one-hot encoding.\n",
    "High Cardinality Categorical: 'name', 'host_name', 'neighbourhood', 'room_type', ‘price_category' all  have a large number of unique categories, so we applied target encoding to avoid high dimensionality.\n",
    "Continuous Features: 'calculated_host_listings_count', 'hospital', 'university' are treated as numeric values based on their value distribution and usage in previous prediction models. ",
    "We used StandardScaler on these numeric features to standardize their magnitude, which is important for many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845b7a5-1bf3-49aa-bf41-0023332f20ec",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "This notebook displays the pre-processing and training data development phase of our Airbnb predictions project. \n",
    "In this notebook we used Dummy Encoding, which applied one-hot encoding to low-cardinality categorical features (disponibility_cat, min_nights_cat, average_measure, supermarket) to prepare them for modeling.\n",
    "Target Encoding: Handled high-cardinality categorical features (name, host_name, neighbourhood, room_type, price_category) using target encoding, which reduces dimensionality while preserving meaningful patterns with respect to the target variable click.\n",
    "Standardization: Scaled continuous numeric features ('calculated_host_listings_count', 'hospital', 'university') using StandardScaler to ensure features contribute equally to model learning.\n",
    "Train-Test Split: Split the dataset into training and testing subsets using an 80/20 ratio while maintaining the target distribution (stratify=y) to ensure fair model evaluation.\n",
    "After completing our preprocessing steps, we now have clean and standardized datasets ready for building and evaluating predictive machine learning models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
